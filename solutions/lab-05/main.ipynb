{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uwaga\n",
    "\n",
    "Korzystam jedynie z dwóch modeli, a nie z trzech jak jest to sformułowane w treści zadania.\n",
    "\n",
    "Wynika to z faktu, że jedynie 2 z pośród 6, które próbowałem, udało mi się uruchomić. Pozostałe, takie jak: `google/mt5-base`, `allegro/plt5-base` wymagały zależności natywnych (`protobuf`), których nie mogłem zainstalować system-wide jako, że pracowałem\n",
    "na sprzęcie na którym nie miałem `roota`. \n",
    "\n",
    "Popatrzę na zajęciach na wnioski innych, natomiast w tym sprawozdaniu będą one oparte tylko na testach przeprowadzonych na dwóch modelach: `papuGaPT2`, `allegro/herbert-base-cased`. \"W zamian\" nieco więcej eksperymentuję na końcu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2\n",
    "\n",
    "\n",
    "Download three Polish models from the Huggingface repository. These should be regular language models, which were not fine-tuned. E.g. HerBERT and papuGaPT2 are good examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkafara/.cache/virtualenvs/python-3.11-venv-ds/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "modelname_allegro_herbert_base_cased = \"allegro/herbert-base-cased\"\n",
    "modelname_papugapt2 = \"flax-community/papuGaPT2\"\n",
    "modelname_google_mt5_base = \"google/mt5-base\"\n",
    "modelname_plt5 = \"allegro/plt5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "papuga_pipe = pipeline(model=modelname_papugapt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.sso.sso_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "herbert_pipe = pipeline(model=modelname_allegro_herbert_base_cased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mt5_pipe = pipeline(model=modelname_google_mt5_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 3\n",
    "\n",
    "Devise a method to test if the langage model understands Polish cases. E.g. testing for nominal case could be expressed as \"Warszawa to największe [MASK]\",\n",
    "and the masked word should be in nominative case. Create sentences for each case.\n",
    "\n",
    "\n",
    "*Zdaję sobie sprawę, że poniżej porównuję różne modele, na różnych zadaniach, ale nie mogłem tak dobrać, żeby wszystkie trzy obsługiwały te same zadania & wyniki były sensowne*. Do tego dokumentacja większości polskich modeli\n",
    "jest tragiczna (mówię tutaj w szczególności o HerBERT, którego docsy są co najwyżej prowizoryczne i absolutinie nie begginer-friendly (mówię o sobie))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crashuje w przypadku tego modelu\n",
    "# papuga_fm = pipeline(task=\"fill-mask\", model=modelname_papugapt2)\n",
    "\n",
    "papuga_tg = pipeline(task=\"text-generation\", model=modelname_papugapt2)\n",
    "herbert_fm = pipeline(task=\"fill-mask\", model=modelname_allegro_herbert_base_cased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mianownik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Stolicą Polski oczywiście jest Warszawa, bo w niej mamy dostęp do wszystkich najważniejszych miejsc i zabytków, które są w Polsce warte zobaczenia. A na pewno Polska jest miejscem, do którego warto przyjechać, jeśli szukamy jakiegoś miejsca widokowego, w końcu po to'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(\"Stolicą Polski oczywiście jest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na wiele różnych (~10) uruchomień powyższej komórki poprawną odpowiedź (Warszawa) otrzymałem tylko 2 razy. Niemniej jednak forma samego wyrazu była zawsze poprawna.\n",
    "\n",
    "Zwracam uwagę też na to, że w promptach nie korzystam z `[MASK]`, ponieważ przykłady w dokumentacji modelu `papuGaPT2` tego nie robią (podejrzewam, że nie ma możliwości maskowania, bo potrafi tylko\n",
    "\"kontynuować zdania\", ale nie mam pewności). Nie wiem też jaki to ma wpływ na wyniki.\n",
    "\n",
    "Fragment dokumentacji do którego się odnoszę: https://huggingface.co/flax-community/papuGaPT2?text=Najsmaczniejszy+polski+owoc+to#text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.36618852615356445,\n",
       "  'token': 5936,\n",
       "  'token_str': 'Warszawa',\n",
       "  'sequence': 'Stolicą Polski oczywiścje jest Warszawa'},\n",
       " {'score': 0.16102533042430878,\n",
       "  'token': 4394,\n",
       "  'token_str': 'Kraków',\n",
       "  'sequence': 'Stolicą Polski oczywiścje jest Kraków'},\n",
       " {'score': 0.12439846247434616,\n",
       "  'token': 1826,\n",
       "  'token_str': '…',\n",
       "  'sequence': 'Stolicą Polski oczywiścje jest …'},\n",
       " {'score': 0.040639717131853104,\n",
       "  'token': 6999,\n",
       "  'token_str': 'Poznań',\n",
       "  'sequence': 'Stolicą Polski oczywiścje jest Poznań'},\n",
       " {'score': 0.038738179951906204,\n",
       "  'token': 9250,\n",
       "  'token_str': 'Wrocław',\n",
       "  'sequence': 'Stolicą Polski oczywiścje jest Wrocław'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Stolicą Polski oczywiścje jest <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku tego modelu (i tego zadania\" `fill-mask`), odpowiedź poprawna jako pierwsza (najwyższy score) była pokazywana za każdym razem. Forma wyrazu też zawsze była poprawna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dopełniacz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Nie będziemy palić w piecu dla dobra całej naszej społeczności. Do tego będziemy palić w obecności i z upoważnienia swoich najbliższych.\\nCzy to z naszej zgody, czy z woli, czy życzenia, to oni sami decydują w jak należy z naszej perspektywy wyglądać.\\n'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(\"Nie będziemy palić w piecu dla dobra całej\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyraz / sekwencja została uzupełniona wyrazami w prawidłowej formie i jako tako z sensem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.28093963861465454,\n",
       "  'token': 4877,\n",
       "  'token_str': 'rodziny',\n",
       "  'sequence': 'Nie będziemy palić w piecu dla dobra całej rodziny'},\n",
       " {'score': 0.15739218890666962,\n",
       "  'token': 2960,\n",
       "  'token_str': 'Polski',\n",
       "  'sequence': 'Nie będziemy palić w piecu dla dobra całej Polski'},\n",
       " {'score': 0.09003997594118118,\n",
       "  'token': 23891,\n",
       "  'token_str': 'ludzkości',\n",
       "  'sequence': 'Nie będziemy palić w piecu dla dobra całej ludzkości'},\n",
       " {'score': 0.07397733628749847,\n",
       "  'token': 4813,\n",
       "  'token_str': 'Europy',\n",
       "  'sequence': 'Nie będziemy palić w piecu dla dobra całej Europy'},\n",
       " {'score': 0.06293289363384247,\n",
       "  'token': 11849,\n",
       "  'token_str': 'społeczności',\n",
       "  'sequence': 'Nie będziemy palić w piecu dla dobra całej społeczności'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Nie będziemy palić w piecu dla dobra całej <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj formy wyrazów są jak najbardziej poprawne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Celownik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Nowy karabin maszynowy zostanie wydany każdemu obywatelowi, nie tylko mieszkańcom USA. Każdy Amerykanin może zapłacić w USA za broń do celów specjalnych, takich jak broń chemiczna lub biologiczna. Amerykański rynek uzbrojony w taki karabin maszynowy będzie'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(\"Nowy karabin maszynowy zostanie wydany każdemu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forma zwykle była poprawna. Ciekawe jest (choć pewnie mało zaskakujące), że gdy przykład jest z bronią, to bardzo często pojawia się USA :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5254270434379578,\n",
       "  'token': 1899,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Nowy karabin maszynowy zostanie wydany każdemu.'},\n",
       " {'score': 0.03937682509422302,\n",
       "  'token': 1725,\n",
       "  'token_str': '!',\n",
       "  'sequence': 'Nowy karabin maszynowy zostanie wydany każdemu!'},\n",
       " {'score': 0.03688626363873482,\n",
       "  'token': 19617,\n",
       "  'token_str': 'dziecku',\n",
       "  'sequence': 'Nowy karabin maszynowy zostanie wydany każdemu dziecku'},\n",
       " {'score': 0.0337052047252655,\n",
       "  'token': 1826,\n",
       "  'token_str': '…',\n",
       "  'sequence': 'Nowy karabin maszynowy zostanie wydany każdemu …'},\n",
       " {'score': 0.032657984644174576,\n",
       "  'token': 1046,\n",
       "  'token_str': 'z',\n",
       "  'sequence': 'Nowy karabin maszynowy zostanie wydany każdemu z'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Nowy karabin maszynowy zostanie wydany każdemu <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj model nieco mnie ograł uzupełniając interpunkcją, ale po outputcie z \"dziecku\" mogę stwierdzić, że dobrał poprawną formę, choć może nie najbardziej sensowną (USA xD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biernik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Widzę w klatce niesamowicie kolorową papużkę, i jak na moją wyobraźnię może mieć całkiem sporo kolorów :))\\nPotrafią zauroczyć'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(\"Widzę w klatce niesamowicie kolorową\", max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papuga przewidziała papugę, fantastycznie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.42295607924461365,\n",
       "  'token': 1899,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Widzę w klatce niesamowicie kolorową.'},\n",
       " {'score': 0.065530925989151,\n",
       "  'token': 22731,\n",
       "  'token_str': 'skórę',\n",
       "  'sequence': 'Widzę w klatce niesamowicie kolorową skórę'},\n",
       " {'score': 0.038813650608062744,\n",
       "  'token': 1725,\n",
       "  'token_str': '!',\n",
       "  'sequence': 'Widzę w klatce niesamowicie kolorową!'},\n",
       " {'score': 0.030762039124965668,\n",
       "  'token': 1826,\n",
       "  'token_str': '…',\n",
       "  'sequence': 'Widzę w klatce niesamowicie kolorową …'},\n",
       " {'score': 0.02667396515607834,\n",
       "  'token': 10319,\n",
       "  'token_str': 'postać',\n",
       "  'sequence': 'Widzę w klatce niesamowicie kolorową postać'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Widzę w klatce niesamowicie kolorową <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pod względem syntaktycznym, semantycznie nieco mniej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narzędnik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Do Tajwanu pojedziemy z jednym z największych, globalnych dystrybutorów produktów z kamienia naturalnego – firmą Kafle Ceramika Sp. z o.o.\\nW Tajpej znajdziemy zarówno duże, popularne miasta („Manhaket”, „Papuana”,'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(\"Do Tajwanu pojedziemy z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1072307899594307,\n",
       "  'token': 1826,\n",
       "  'token_str': '…',\n",
       "  'sequence': 'Do Tajwany pojedziemy z …'},\n",
       " {'score': 0.04334069415926933,\n",
       "  'token': 1335,\n",
       "  'token_str': ':',\n",
       "  'sequence': 'Do Tajwany pojedziemy z :'},\n",
       " {'score': 0.03576905280351639,\n",
       "  'token': 10424,\n",
       "  'token_str': 'dziećmi',\n",
       "  'sequence': 'Do Tajwany pojedziemy z dziećmi'},\n",
       " {'score': 0.03332872316241264,\n",
       "  'token': 13128,\n",
       "  'token_str': 'dzieckiem',\n",
       "  'sequence': 'Do Tajwany pojedziemy z dzieckiem'},\n",
       " {'score': 0.03192038834095001,\n",
       "  'token': 6094,\n",
       "  'token_str': 'Warszawy',\n",
       "  'sequence': 'Do Tajwany pojedziemy z Warszawy'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Do Tajwany pojedziemy z <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miejscownik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Media przez ostatni tydzień nie przestawały mówić o kolejnych protestach górniczych, które zostały rozpoczęte po tym, jak górnicy zdecydowali się na podwyżki płac. Jednocześnie były prowadzone rozmowy na temat wspólnego wyjścia z kopalni i przejścia na węgiel. W marcu media obie'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(\"Media przez ostatni tydzień nie przestawały mówić o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poprawna forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5119613409042358,\n",
       "  'token': 1826,\n",
       "  'token_str': '…',\n",
       "  'sequence': 'Media przez ostatni tydzień nie przestawyały mówić o …'},\n",
       " {'score': 0.013213361613452435,\n",
       "  'token': 1899,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Media przez ostatni tydzień nie przestawyały mówić o.'},\n",
       " {'score': 0.012496136128902435,\n",
       "  'token': 1335,\n",
       "  'token_str': ':',\n",
       "  'sequence': 'Media przez ostatni tydzień nie przestawyały mówić o :'},\n",
       " {'score': 0.012240143492817879,\n",
       "  'token': 21144,\n",
       "  'token_str': 'problemach',\n",
       "  'sequence': 'Media przez ostatni tydzień nie przestawyały mówić o problemach'},\n",
       " {'score': 0.011556684039533138,\n",
       "  'token': 39319,\n",
       "  'token_str': 'kryzysie',\n",
       "  'sequence': 'Media przez ostatni tydzień nie przestawyały mówić o kryzysie'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Media przez ostatni tydzień nie przestawyały mówić o <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Słabe są te interpunkcje, ale ostatnie 2 tokeny są w porządku syntaktycznie i semantycznie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wołacz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Litwo! Ojczyzno moja” był bardzo ciekawy. Nie mogliśmy się nadziwić jakim wielkim znawcą poezji był nasz kolega, czy też pani Grażyna i co o niej wiemy, jakie ma ona dla Polski znaczenie. Ucieszyliśmy się z tego'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(\"Litwo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie za bardzo przychodzi mi do głowy przykłąd z wołaczem na końcu zdania :D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3647006154060364,\n",
       "  'token': 14080,\n",
       "  'token_str': 'Witaj',\n",
       "  'sequence': 'Witaj! Ojczyzno moja'},\n",
       " {'score': 0.051284655928611755,\n",
       "  'token': 21136,\n",
       "  'token_str': 'Hej',\n",
       "  'sequence': 'Hej! Ojczyzno moja'},\n",
       " {'score': 0.04865175113081932,\n",
       "  'token': 1063,\n",
       "  'token_str': 'O',\n",
       "  'sequence': 'O! Ojczyzno moja'},\n",
       " {'score': 0.04752074554562569,\n",
       "  'token': 10303,\n",
       "  'token_str': 'Pozdrawiam',\n",
       "  'sequence': 'Pozdrawiam! Ojczyzno moja'},\n",
       " {'score': 0.04677436500787735,\n",
       "  'token': 15406,\n",
       "  'token_str': 'Cześć',\n",
       "  'sequence': 'Cześć! Ojczyzno moja'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"<mask>! Ojczyzno moja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie za bardzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 4\n",
    "\n",
    "Devise a method to test long-range relationships such as gender. E.e. you can use two verbs with masculine and feminine gender, where one of the verbs is masked. Both verbs should have the same gender, assuming the subject is the same. Define at least 3 such sentences.\n",
    "\n",
    "Jako, że `papuGaPT2` nie ma możliwości `fill-mask` (a przynajmniej nie znalazłem), to będę bardziej różnicował przykłady pomiedzy modelami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Płotka, klacz Geralta, wcale nie owiana tajemnicą, a z oczywistych powodów bardzo rzadka na całej planecie, a i w świecie Gwiezdnych Wojen nie była przedmiotem podziwu i zazdrości wśród zwolenników tego typu gier. I tak'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(\"Płotka, klacz Geralta, wcale nie \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że model \"wywnioskował\" płeć i konsekwentnie się jej trzyma. Możemy spróbować jeszcze z mniejszą liczbą informacji (bo tutaj jest zarówno \"Płotka\", jak i \"klacz\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Płotka, jako wierny wierzchowiec Geralta, wcale nie jest taki zły.\\n„Gawiedź” opowiada też prawdziwą historię rodziny Obi-Wana Kenobi, która nie dość, że nie miała z kim zostawić córki,'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(\"Płotka, jako wierny wierzchowiec Geralta, wcale nie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz już popłynął. I coś go bardzo ciągnie do \"Star Wars\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Klacz Geralta wcale nie owiana była takim urokiem jak inni, lecz jego ojciec z właściwą sobie lekkomyślnością, mimo braku zainteresowania i chęci do działania, a to wszystko za sprawą swoich dwóch córek!\\nNależąca do'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(\"Klacz Geralta wcale nie \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^-- powyżej widzimy, że podmiot został już zgubiony, a to zdanie nie jest złożone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4900616705417633,\n",
       "  'token': 3583,\n",
       "  'token_str': 'musi',\n",
       "  'sequence': 'Płotka, klacz Geralta, wcale nie musi tego robić, ani tym bardziej przejmować się'},\n",
       " {'score': 0.0974636971950531,\n",
       "  'token': 5578,\n",
       "  'token_str': 'powinna',\n",
       "  'sequence': 'Płotka, klacz Geralta, wcale nie powinna tego robić, ani tym bardziej przejmować się'},\n",
       " {'score': 0.06618716567754745,\n",
       "  'token': 3633,\n",
       "  'token_str': 'chce',\n",
       "  'sequence': 'Płotka, klacz Geralta, wcale nie chce tego robić, ani tym bardziej przejmować się'},\n",
       " {'score': 0.05556068569421768,\n",
       "  'token': 10437,\n",
       "  'token_str': 'chciała',\n",
       "  'sequence': 'Płotka, klacz Geralta, wcale nie chciała tego robić, ani tym bardziej przejmować się'},\n",
       " {'score': 0.05114469677209854,\n",
       "  'token': 8301,\n",
       "  'token_str': 'zamierza',\n",
       "  'sequence': 'Płotka, klacz Geralta, wcale nie zamierza tego robić, ani tym bardziej przejmować się'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Płotka, klacz Geralta, wcale nie <mask> tego robić, ani tym bardziej przejmować się \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^-- Wygląda to solidnie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.629269003868103,\n",
       "  'token': 23086,\n",
       "  'token_str': 'ruszyła',\n",
       "  'sequence': 'Klacz Geralta zreflektowała się szybko i ruszyła w jego stronę, natomiast on nie mógł wstać'},\n",
       " {'score': 0.12875309586524963,\n",
       "  'token': 19464,\n",
       "  'token_str': 'poszła',\n",
       "  'sequence': 'Klacz Geralta zreflektowała się szybko i poszła w jego stronę, natomiast on nie mógł wstać'},\n",
       " {'score': 0.05777115747332573,\n",
       "  'token': 25094,\n",
       "  'token_str': 'rzuciła',\n",
       "  'sequence': 'Klacz Geralta zreflektowała się szybko i rzuciła w jego stronę, natomiast on nie mógł wstać'},\n",
       " {'score': 0.023694951087236404,\n",
       "  'token': 45675,\n",
       "  'token_str': 'uciekła',\n",
       "  'sequence': 'Klacz Geralta zreflektowała się szybko i uciekła w jego stronę, natomiast on nie mógł wstać'},\n",
       " {'score': 0.023675614967942238,\n",
       "  'token': 41642,\n",
       "  'token_str': 'skoczyła',\n",
       "  'sequence': 'Klacz Geralta zreflektowała się szybko i skoczyła w jego stronę, natomiast on nie mógł wstać'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Klacz Geralta zreflektowała się szybko i <mask> w jego stronę, natomiast on nie mógł wstać\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.579107403755188,\n",
       "  'token': 17489,\n",
       "  'token_str': 'zatrzymała',\n",
       "  'sequence': 'Klacz Geralta zatrzymała się szybko i pobiegła w jego stronę, natomiast on nie mógł wstać'},\n",
       " {'score': 0.1394488662481308,\n",
       "  'token': 40910,\n",
       "  'token_str': 'podniosła',\n",
       "  'sequence': 'Klacz Geralta podniosła się szybko i pobiegła w jego stronę, natomiast on nie mógł wstać'},\n",
       " {'score': 0.0818558931350708,\n",
       "  'token': 12142,\n",
       "  'token_str': 'pojawiła',\n",
       "  'sequence': 'Klacz Geralta pojawiła się szybko i pobiegła w jego stronę, natomiast on nie mógł wstać'},\n",
       " {'score': 0.026939714327454567,\n",
       "  'token': 23086,\n",
       "  'token_str': 'ruszyła',\n",
       "  'sequence': 'Klacz Geralta ruszyła się szybko i pobiegła w jego stronę, natomiast on nie mógł wstać'},\n",
       " {'score': 0.015271407552063465,\n",
       "  'token': 10061,\n",
       "  'token_str': 'znalazła',\n",
       "  'sequence': 'Klacz Geralta znalazła się szybko i pobiegła w jego stronę, natomiast on nie mógł wstać'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Klacz Geralta <mask> się szybko i pobiegła w jego stronę, natomiast on nie mógł wstać\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HerBERT tutaj radzi sobie lepiej z formami czasowników. Nawet gdy w zdaniu występuję \"drugi potencjalny podmiot\". Spróbujmy jeszcze mu skomplikować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2289886474609375,\n",
       "  'token': 13792,\n",
       "  'token_str': 'chcesz',\n",
       "  'sequence': 'Kupiłabym Ci prezent, lecz ostatnio zachowujesz się niemile, dlatego nie chcesz ode mnie nic w tym roku, a już na pewno nie podaruję Ci'},\n",
       " {'score': 0.13760817050933838,\n",
       "  'token': 8303,\n",
       "  'token_str': 'masz',\n",
       "  'sequence': 'Kupiłabym Ci prezent, lecz ostatnio zachowujesz się niemile, dlatego nie masz ode mnie nic w tym roku, a już na pewno nie podaruję Ci'},\n",
       " {'score': 0.08001423627138138,\n",
       "  'token': 39407,\n",
       "  'token_str': 'dostałam',\n",
       "  'sequence': 'Kupiłabym Ci prezent, lecz ostatnio zachowujesz się niemile, dlatego nie dostałam ode mnie nic w tym roku, a już na pewno nie podaruję Ci'},\n",
       " {'score': 0.060660675168037415,\n",
       "  'token': 34355,\n",
       "  'token_str': 'bierz',\n",
       "  'sequence': 'Kupiłabym Ci prezent, lecz ostatnio zachowujesz się niemile, dlatego nie bierz ode mnie nic w tym roku, a już na pewno nie podaruję Ci'},\n",
       " {'score': 0.04980644956231117,\n",
       "  'token': 43800,\n",
       "  'token_str': 'odbierze',\n",
       "  'sequence': 'Kupiłabym Ci prezent, lecz ostatnio zachowujesz się niemile, dlatego nie odbierze ode mnie nic w tym roku, a już na pewno nie podaruję Ci'}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Kupiłabym Ci prezent, lecz ostatnio zachowujesz się niemile, dlatego nie <mask> ode mnie nic w tym roku, a już na pewno nie podaruję Ci\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu już się pogubił. Widzimy, że forma czasownika nie jest poprawnie dopasowana do kontekstu: \"dostałam\", \"odbierze\", itd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 5\n",
    "\n",
    "Check if the model captures real-world knolwedge. For instance a sentence \"[MASK] wrze w temperaturze 100 stopni, a zamarza w temperaturze 0 stopni Celsjusza.\" checks if the model \"knows\" the description of water. Define at least 3 such sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Krzysztof Krawczyk był gościem pierwszego odcinka programu “Masters of Computer & Internet” na antenie Dwójki. Zadał tam pytanie, czy chce prowadzić własne badania naukowe. Dziennikarze pytali go, czy uważa, że tak.\\n–'}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(\"Krzysztof Krawczyk był\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fatalna wiedza... ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Koronacja Napoleona na cesarza miała miejsce drugiego grudnia roku \\xad1815, a więc już po śmierci cesarza. Jednakże tego samego dnia Bonaparte przybył do swojego miasta i z powodu wojny, do której zapadł, miasto na kilka lat straciło'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(\"Koronacja Napoleona na cesarza miała miejsce drugiego grudnia roku \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No niestety nie można zgodzić się z tymi fantazjami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2605268955230713,\n",
       "  'token': 1826,\n",
       "  'token_str': '…',\n",
       "  'sequence': 'Krzysztof Krawczyk był …'},\n",
       " {'score': 0.03785598650574684,\n",
       "  'token': 1019,\n",
       "  'token_str': 'w',\n",
       "  'sequence': 'Krzysztof Krawczyk był w'},\n",
       " {'score': 0.025451788678765297,\n",
       "  'token': 8720,\n",
       "  'token_str': 'najlepszy',\n",
       "  'sequence': 'Krzysztof Krawczyk był najlepszy'},\n",
       " {'score': 0.023693649098277092,\n",
       "  'token': 1899,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Krzysztof Krawczyk był.'},\n",
       " {'score': 0.02130425162613392,\n",
       "  'token': 1335,\n",
       "  'token_str': ':',\n",
       "  'sequence': 'Krzysztof Krawczyk był :'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Krzysztof Krawczyk był <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"był najlepszy\" <-- z tym możemy się zgodzić ;D Gdyby udało się wyeliminować tą interpunkcję, to byłoby całkiem nieźle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.768883228302002,\n",
       "  'token': 1899,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Koronacja Napoleona na cesarza miałą miejsce drugiego grudnia roku.'},\n",
       " {'score': 0.018007507547736168,\n",
       "  'token': 42312,\n",
       "  'token_str': '1815',\n",
       "  'sequence': 'Koronacja Napoleona na cesarza miałą miejsce drugiego grudnia roku 1815'},\n",
       " {'score': 0.01520566176623106,\n",
       "  'token': 1826,\n",
       "  'token_str': '…',\n",
       "  'sequence': 'Koronacja Napoleona na cesarza miałą miejsce drugiego grudnia roku …'},\n",
       " {'score': 0.01120938640087843,\n",
       "  'token': 40087,\n",
       "  'token_str': '1863',\n",
       "  'sequence': 'Koronacja Napoleona na cesarza miałą miejsce drugiego grudnia roku 1863'},\n",
       " {'score': 0.011038212105631828,\n",
       "  'token': 48876,\n",
       "  'token_str': '1870',\n",
       "  'sequence': 'Koronacja Napoleona na cesarza miałą miejsce drugiego grudnia roku 1870'}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Koronacja Napoleona na cesarza miałą miejsce drugiego grudnia roku <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No niestety, nie ma tutaj poprawnej odpowiedzi (1804).\n",
    "\n",
    "Dam jeszcze jedna szansę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.04818161949515343,\n",
       "  'token': 14012,\n",
       "  'token_str': 'John',\n",
       "  'sequence': 'John Gagarin był pierwszym człowiekiem w kosmosie'},\n",
       " {'score': 0.045256227254867554,\n",
       "  'token': 18154,\n",
       "  'token_str': 'George',\n",
       "  'sequence': 'George Gagarin był pierwszym człowiekiem w kosmosie'},\n",
       " {'score': 0.04135838896036148,\n",
       "  'token': 8269,\n",
       "  'token_str': 'Aleksander',\n",
       "  'sequence': 'Aleksander Gagarin był pierwszym człowiekiem w kosmosie'},\n",
       " {'score': 0.033182237297296524,\n",
       "  'token': 22456,\n",
       "  'token_str': 'Albert',\n",
       "  'sequence': 'Albert Gagarin był pierwszym człowiekiem w kosmosie'},\n",
       " {'score': 0.032033007591962814,\n",
       "  'token': 31195,\n",
       "  'token_str': 'Charles',\n",
       "  'sequence': 'Charles Gagarin był pierwszym człowiekiem w kosmosie'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"<mask> Gagarin był pierwszym człowiekiem w kosmosie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yuri by się zmartwił. Chociaż może to mało \"polskie\" zdanie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.12764084339141846,\n",
       "  'token': 39710,\n",
       "  'token_str': 'Drzewiecki',\n",
       "  'sequence': 'Mirosław Drzewiecki był pierwszym i dotychczas jedynym Polakiem, który odbył lot w kosmos.'},\n",
       " {'score': 0.029964834451675415,\n",
       "  'token': 24394,\n",
       "  'token_str': 'Jabłoński',\n",
       "  'sequence': 'Mirosław Jabłoński był pierwszym i dotychczas jedynym Polakiem, który odbył lot w kosmos.'},\n",
       " {'score': 0.027027016505599022,\n",
       "  'token': 19421,\n",
       "  'token_str': 'Wiśniewski',\n",
       "  'sequence': 'Mirosław Wiśniewski był pierwszym i dotychczas jedynym Polakiem, który odbył lot w kosmos.'},\n",
       " {'score': 0.023340020328760147,\n",
       "  'token': 26265,\n",
       "  'token_str': 'Witkowski',\n",
       "  'sequence': 'Mirosław Witkowski był pierwszym i dotychczas jedynym Polakiem, który odbył lot w kosmos.'},\n",
       " {'score': 0.020443130284547806,\n",
       "  'token': 24285,\n",
       "  'token_str': 'Michalski',\n",
       "  'sequence': 'Mirosław Michalski był pierwszym i dotychczas jedynym Polakiem, który odbył lot w kosmos.'}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Mirosław <mask> był pierwszym i dotychczas jedynym Polakiem, który odbył lot w kosmos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Niestety nie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.012200584635138512,\n",
       "  'token': 4895,\n",
       "  'token_str': '500',\n",
       "  'sequence': 'Najwyższa góra na świecie ma wysokość 500 metrów ponad poziomem morza, natomiast najwyższy szczyt Europy, góra Elbrus, ma zaledwie 5, 642 metrów ponad poziomem morza'},\n",
       " {'score': 0.01208607479929924,\n",
       "  'token': 7569,\n",
       "  'token_str': '600',\n",
       "  'sequence': 'Najwyższa góra na świecie ma wysokość 600 metrów ponad poziomem morza, natomiast najwyższy szczyt Europy, góra Elbrus, ma zaledwie 5, 642 metrów ponad poziomem morza'},\n",
       " {'score': 0.01175074651837349,\n",
       "  'token': 48756,\n",
       "  'token_str': '6000',\n",
       "  'sequence': 'Najwyższa góra na świecie ma wysokość 6000 metrów ponad poziomem morza, natomiast najwyższy szczyt Europy, góra Elbrus, ma zaledwie 5, 642 metrów ponad poziomem morza'},\n",
       " {'score': 0.010753924958407879,\n",
       "  'token': 3450,\n",
       "  'token_str': '2000',\n",
       "  'sequence': 'Najwyższa góra na świecie ma wysokość 2000 metrów ponad poziomem morza, natomiast najwyższy szczyt Europy, góra Elbrus, ma zaledwie 5, 642 metrów ponad poziomem morza'},\n",
       " {'score': 0.010630584321916103,\n",
       "  'token': 21553,\n",
       "  'token_str': '3000',\n",
       "  'sequence': 'Najwyższa góra na świecie ma wysokość 3000 metrów ponad poziomem morza, natomiast najwyższy szczyt Europy, góra Elbrus, ma zaledwie 5, 642 metrów ponad poziomem morza'}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(\"Najwyższa góra na świecie ma wysokość <mask> metrów ponad poziomem morza, natomiast najwyższy szczyt Europy, góra Elbrus, ma zaledwie 5,642 metrów ponad poziomem morza\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Też nie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 6\n",
    "\n",
    "Check zero-shot learning capabilites of the models. Provide at least 5 sentences with different sentiment for the following scheme: \"'Ten film to był kiler. Nie mogłem się oderwać od ekranu.' Wypowiedź ta ma jest zdecydowanie [MASK]\" Try different prompts, to see if they make any difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skoro `zero-shot` to te zdania podaję jako osobne prompty, nie łącznie\n",
    "\n",
    "Czytając docsy `papuGaPT2` odniosłem wrażenie, że `zero-shot learning` to po prostu przechowywanie wiedzy przez model (screenshot poniżej)\n",
    "\n",
    "![Zero-shot learning by papuga](image.png)\n",
    "\n",
    "Co nieco kłóci mi się z bardziej znaną mi defnicją: \n",
    "\n",
    "> Zero-shot learning (ZSL) is a problem setup in deep learning where, at test time, a learner observes samples from classes which were not observed during training,\n",
    "> and needs to predict the class that they belong to. Zero-shot methods generally work by associating observed and non-observed classes through some form of auxiliary information,\n",
    "> which encodes observable distinguishing properties of objects.\n",
    "\n",
    "(cytat z wiki)\n",
    "\n",
    "\n",
    "Pójdę za definicją z wiki & zasugerowanym przykładem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Poziom usług medycznych w przychodniach Scanmed jest wprost niesamowity. Wydźwięk tej wypowiedzi jest zdecydowanie pozytywny. Pacjenci mogą liczyć nie tylko na profesjonalne podejście wykwalifikowanych lekarzy, ale również na opiekę medyczną najwyższej jakości.\\nNasza przychodnia oferuje konsultacje lekarzy specjalistów zajmujących'}]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_1 = \"Poziom usług medycznych w przychodniach Scanmed jest wprost niesamowity. Wydźwięk tej wypowiedzi jest zdecydowanie\"\n",
    "papuga_tg(prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3778590261936188,\n",
       "  'token': 28531,\n",
       "  'token_str': 'pozytywny',\n",
       "  'sequence': 'Poziom usług medycznych w przychodniach Scanmed jest wprost niesamowity. Wydźwięk tej wypowiedzi jest zdecydowanie pozytywny'},\n",
       " {'score': 0.08948273956775665,\n",
       "  'token': 11470,\n",
       "  'token_str': 'lepszy',\n",
       "  'sequence': 'Poziom usług medycznych w przychodniach Scanmed jest wprost niesamowity. Wydźwięk tej wypowiedzi jest zdecydowanie lepszy'},\n",
       " {'score': 0.08056753873825073,\n",
       "  'token': 1899,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Poziom usług medycznych w przychodniach Scanmed jest wprost niesamowity. Wydźwięk tej wypowiedzi jest zdecydowanie.'},\n",
       " {'score': 0.045824114233255386,\n",
       "  'token': 1826,\n",
       "  'token_str': '…',\n",
       "  'sequence': 'Poziom usług medycznych w przychodniach Scanmed jest wprost niesamowity. Wydźwięk tej wypowiedzi jest zdecydowanie …'},\n",
       " {'score': 0.030652863904833794,\n",
       "  'token': 17221,\n",
       "  'token_str': 'wyższy',\n",
       "  'sequence': 'Poziom usług medycznych w przychodniach Scanmed jest wprost niesamowity. Wydźwięk tej wypowiedzi jest zdecydowanie wyższy'}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(prompt_1 + \" <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oba modele dały radę, ale może być tak, że fraza \"wydźwięk tej wypowiedzi jest zdecydowanie\" jest bardzo \"sugerująca\" dla modelu i odpowiednie słowo jest wstawione tylko na podstawie tego kontekstu, bez uwzględnienia \"sentymentu\" całej wypowiedzi.\n",
    "Sprawdźmy to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Poziom usług medycznych w przychodniach Scanmed jest wprost żenujący. Wydźwięk tej wypowiedzi jest zdecydowanie negatywny.\\n„Jeśli się przyjmą, to będzie się ich tak długo uczył, dopóki będą miały jakieś kłopoty” – takie zdania padają najczęściej.'}]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_2 = \"Poziom usług medycznych w przychodniach Scanmed jest wprost żenujący. Wydźwięk tej wypowiedzi jest zdecydowanie\"\n",
    "papuga_tg(prompt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2659365236759186,\n",
       "  'token': 39051,\n",
       "  'token_str': 'negatywny',\n",
       "  'sequence': 'Poziom usług medycznych w przychodniach Scanmed jest wprost żenujący. Wydźwięk tej wypowiedzi jest zdecydowanie negatywny'},\n",
       " {'score': 0.11085037887096405,\n",
       "  'token': 1899,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Poziom usług medycznych w przychodniach Scanmed jest wprost żenujący. Wydźwięk tej wypowiedzi jest zdecydowanie.'},\n",
       " {'score': 0.08360789716243744,\n",
       "  'token': 25812,\n",
       "  'token_str': 'gorszy',\n",
       "  'sequence': 'Poziom usług medycznych w przychodniach Scanmed jest wprost żenujący. Wydźwięk tej wypowiedzi jest zdecydowanie gorszy'},\n",
       " {'score': 0.0739324688911438,\n",
       "  'token': 13028,\n",
       "  'token_str': 'zły',\n",
       "  'sequence': 'Poziom usług medycznych w przychodniach Scanmed jest wprost żenujący. Wydźwięk tej wypowiedzi jest zdecydowanie zły'},\n",
       " {'score': 0.04786384850740433,\n",
       "  'token': 1826,\n",
       "  'token_str': '…',\n",
       "  'sequence': 'Poziom usług medycznych w przychodniach Scanmed jest wprost żenujący. Wydźwięk tej wypowiedzi jest zdecydowanie …'}]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(prompt_2 + \" <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 / 10 uruchomień komórki z \"papugą\", odpowiedź była: \"negatywny\" <-- więc jednak można wysuwać nieśmiałe wnioski, że model potrafi przewidzieć tą wybraną klasę na podstawie kontekstu.\n",
    "\n",
    "\"herbert\" poprawnie odczytywał moje intencje.\n",
    "\n",
    "Popatrzmy na kolejne prompty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wydźwięk \"neutralny\"\n",
    "prompt_3 = \"Trasa autobusu 159 rozpoczyna się w Cichym Kąciku, przebiega przez liczne przystanki, aby ostatecznie zakończyć swój bieg na Mistrzejowicach. Wydźwięk tej wypowiedzi jest zdecydowanie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Trasa autobusu 159 rozpoczyna się w Cichym Kąciku, przebiega przez liczne przystanki, aby ostatecznie zakończyć swój bieg na Mistrzejowicach. Wydźwięk tej wypowiedzi jest zdecydowanie negatywny – to znak, że trzeba zmienić formę aktywności obywatelskiej w naszym mieście.'}]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(prompt_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/10 - pozytywny, pozostałe negatywny, brak odpowiedzi \"neutralny\" (bądź czegoś podobnego)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.17054495215415955,\n",
       "  'token': 28531,\n",
       "  'token_str': 'pozytywny',\n",
       "  'sequence': 'Trasa autobusu 159 rozpoczyna się w Cichym Kąciku, przebiega przez liczne przystanki, aby ostatecznie zakończyć swój bieg na Mistrzejowicach. Wydźwięk tej wypowiedzi jest zdecydowanie pozytywny'},\n",
       " {'score': 0.13442207872867584,\n",
       "  'token': 39051,\n",
       "  'token_str': 'negatywny',\n",
       "  'sequence': 'Trasa autobusu 159 rozpoczyna się w Cichym Kąciku, przebiega przez liczne przystanki, aby ostatecznie zakończyć swój bieg na Mistrzejowicach. Wydźwięk tej wypowiedzi jest zdecydowanie negatywny'},\n",
       " {'score': 0.1079329177737236,\n",
       "  'token': 1899,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Trasa autobusu 159 rozpoczyna się w Cichym Kąciku, przebiega przez liczne przystanki, aby ostatecznie zakończyć swój bieg na Mistrzejowicach. Wydźwięk tej wypowiedzi jest zdecydowanie.'},\n",
       " {'score': 0.07012031227350235,\n",
       "  'token': 5868,\n",
       "  'token_str': 'inny',\n",
       "  'sequence': 'Trasa autobusu 159 rozpoczyna się w Cichym Kąciku, przebiega przez liczne przystanki, aby ostatecznie zakończyć swój bieg na Mistrzejowicach. Wydźwięk tej wypowiedzi jest zdecydowanie inny'},\n",
       " {'score': 0.04975728318095207,\n",
       "  'token': 1335,\n",
       "  'token_str': ':',\n",
       "  'sequence': 'Trasa autobusu 159 rozpoczyna się w Cichym Kąciku, przebiega przez liczne przystanki, aby ostatecznie zakończyć swój bieg na Mistrzejowicach. Wydźwięk tej wypowiedzi jest zdecydowanie :'}]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(prompt_3 + \" <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj też pozytywny / negatywny. Wygląda, że przewidywane są tylko dwie klasy. Może dodam więcej kontekstu, z przykładem dla trzeciej klasy (ale to sprawia, że w kontekście pojawia się więcej wypowiedzi i ciężej może być ocenić jej wydźwięk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_4 = prompt_3 +  \" neutralny. Podobnie będzie, jeżeli powiemy, że trasa linii 20 rozpoczyna się w Cichym Kąciku i biegnie aż na Mały Płaszów. Wtedy wydźwięk tej wypowiedzi jest zdecydowanie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Trasa autobusu 159 rozpoczyna się w Cichym Kąciku, przebiega przez liczne przystanki, aby ostatecznie zakończyć swój bieg na Mistrzejowicach. Wydźwięk tej wypowiedzi jest zdecydowanie neutralny. Podobnie będzie, jeżeli powiemy, że trasa linii 20 rozpoczyna się w Cichym Kąciku i biegnie aż na Mały Płaszów. Wtedy wydźwięk tej wypowiedzi jest zdecydowanie pozytywny.\\nZważywszy na fakt faktu, że linie zostały zawieszone w ramach umowy z KPRP z dniem 31 marca 2016 r.'}]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(prompt_4, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2704119086265564,\n",
       "  'token': 1826,\n",
       "  'token_str': '…',\n",
       "  'sequence': 'Trasa autobusu 159 rozpoczyna się w Cichym Kąciku, przebiega przez liczne przystanki, aby ostatecznie zakończyć swój bieg na Mistrzejowicach. Wydźwięk tej wypowiedzi jest zdecydowanie neutralny. Podobnie będzie, jeżeli powiemy, że trasa linii 20 rozpoczyna się w Cichym Kąciku i biegnie aż na Mały Płaszów. Wtedy wydźwięk tej wypowiedzi jest zdecydowanie …'},\n",
       " {'score': 0.14628084003925323,\n",
       "  'token': 39051,\n",
       "  'token_str': 'negatywny',\n",
       "  'sequence': 'Trasa autobusu 159 rozpoczyna się w Cichym Kąciku, przebiega przez liczne przystanki, aby ostatecznie zakończyć swój bieg na Mistrzejowicach. Wydźwięk tej wypowiedzi jest zdecydowanie neutralny. Podobnie będzie, jeżeli powiemy, że trasa linii 20 rozpoczyna się w Cichym Kąciku i biegnie aż na Mały Płaszów. Wtedy wydźwięk tej wypowiedzi jest zdecydowanie negatywny'},\n",
       " {'score': 0.1226472333073616,\n",
       "  'token': 1899,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Trasa autobusu 159 rozpoczyna się w Cichym Kąciku, przebiega przez liczne przystanki, aby ostatecznie zakończyć swój bieg na Mistrzejowicach. Wydźwięk tej wypowiedzi jest zdecydowanie neutralny. Podobnie będzie, jeżeli powiemy, że trasa linii 20 rozpoczyna się w Cichym Kąciku i biegnie aż na Mały Płaszów. Wtedy wydźwięk tej wypowiedzi jest zdecydowanie.'},\n",
       " {'score': 0.053135257214307785,\n",
       "  'token': 5868,\n",
       "  'token_str': 'inny',\n",
       "  'sequence': 'Trasa autobusu 159 rozpoczyna się w Cichym Kąciku, przebiega przez liczne przystanki, aby ostatecznie zakończyć swój bieg na Mistrzejowicach. Wydźwięk tej wypowiedzi jest zdecydowanie neutralny. Podobnie będzie, jeżeli powiemy, że trasa linii 20 rozpoczyna się w Cichym Kąciku i biegnie aż na Mały Płaszów. Wtedy wydźwięk tej wypowiedzi jest zdecydowanie inny'},\n",
       " {'score': 0.05020315945148468,\n",
       "  'token': 28531,\n",
       "  'token_str': 'pozytywny',\n",
       "  'sequence': 'Trasa autobusu 159 rozpoczyna się w Cichym Kąciku, przebiega przez liczne przystanki, aby ostatecznie zakończyć swój bieg na Mistrzejowicach. Wydźwięk tej wypowiedzi jest zdecydowanie neutralny. Podobnie będzie, jeżeli powiemy, że trasa linii 20 rozpoczyna się w Cichym Kąciku i biegnie aż na Mały Płaszów. Wtedy wydźwięk tej wypowiedzi jest zdecydowanie pozytywny'}]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(prompt_4 + \" <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Znów tylko pozytywny / negatywny.\n",
    "\n",
    "Więc o ile modele te mniej więcej potrafią odczytać te dwie klasy, to wymaganie rozpoznania nowej, wcześniej nie obserwowanej, to dla nich za dużo (przynajmniej w przypadku promptów które zaproponowałem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_5 = \"Wykład profesora Iksińskiego był fascynujący, jestem zachwycony tym, że można opowiadać w tak angażujący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Wykład profesora Iksińskiego był fascynujący, jestem zachwycony tym, że można opowiadać w tak angażujący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie pozytywny. Wykład profesora Iksińskiego jest godny polecenia nauczycielom.\\nBardzo dziękuję za recenzję tego wykładu'}]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(prompt_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.28318530321121216,\n",
       "  'token': 28531,\n",
       "  'token_str': 'pozytywny',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był fascynujący, jestem zachwycony tym, że można opowiadać w tak angażujący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie pozytywny'},\n",
       " {'score': 0.10273756831884384,\n",
       "  'token': 1826,\n",
       "  'token_str': '…',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był fascynujący, jestem zachwycony tym, że można opowiadać w tak angażujący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie …'},\n",
       " {'score': 0.07327660918235779,\n",
       "  'token': 11470,\n",
       "  'token_str': 'lepszy',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był fascynujący, jestem zachwycony tym, że można opowiadać w tak angażujący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie lepszy'},\n",
       " {'score': 0.05549129843711853,\n",
       "  'token': 1899,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był fascynujący, jestem zachwycony tym, że można opowiadać w tak angażujący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie.'},\n",
       " {'score': 0.030925409868359566,\n",
       "  'token': 5868,\n",
       "  'token_str': 'inny',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był fascynujący, jestem zachwycony tym, że można opowiadać w tak angażujący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie inny'}]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(prompt_5 + \" <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj poprawnie odczytany sentyment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_6 = \"Wykład profesora Iksińskiego był do nieczego, jestem zdumiony tym, że można opowiadać w tak nudzący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Wykład profesora Iksińskiego był do nieczego, jestem zdumiony tym, że można opowiadać w tak nudzący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie inny, jeśli w ogóle. Jeżeli nakreślimy na końcu języka jakiś wyob'}]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(prompt_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O, widzimy tutaj, że pozytywny / negatywny to nie jedyne co `papuga` potrafi wstawić"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.12719668447971344,\n",
       "  'token': 1826,\n",
       "  'token_str': '…',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był do nieczego, jestem zdumiony tym, że można opowiadać w tak nudzący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie …'},\n",
       " {'score': 0.11666170507669449,\n",
       "  'token': 1899,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był do nieczego, jestem zdumiony tym, że można opowiadać w tak nudzący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie.'},\n",
       " {'score': 0.10588186234235764,\n",
       "  'token': 5868,\n",
       "  'token_str': 'inny',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był do nieczego, jestem zdumiony tym, że można opowiadać w tak nudzący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie inny'},\n",
       " {'score': 0.09265244752168655,\n",
       "  'token': 39051,\n",
       "  'token_str': 'negatywny',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był do nieczego, jestem zdumiony tym, że można opowiadać w tak nudzący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie negatywny'},\n",
       " {'score': 0.04550796374678612,\n",
       "  'token': 25812,\n",
       "  'token_str': 'gorszy',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był do nieczego, jestem zdumiony tym, że można opowiadać w tak nudzący słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie gorszy'}]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(prompt_6 + \" <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj odpowiedź jest ok. \n",
    "\n",
    "Jeszcze popatrzmy na wypowiedź mieszaną:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_7 = \"Wykład profesora Iksińskiego był do nieczego, jestem zachwycony tym, że można opowiadać w tak obojętny dla słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Wykład profesora Iksińskiego był do nieczego, jestem zachwycony tym, że można opowiadać w tak obojętny dla słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie do przyjęcia, chociaż w przypadku języka mówionego jest on trochę bardziej wyważony.'}]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papuga_tg(prompt_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj model najczęściej nie klasyfikował na `pozytywny negatywny` tylko uzupełniał inną fantazją."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.1544850617647171,\n",
       "  'token': 1826,\n",
       "  'token_str': '…',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był do nieczego, jestem zachwycony tym, że można opowiadać w tak obojętny dla słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie …'},\n",
       " {'score': 0.13079506158828735,\n",
       "  'token': 28531,\n",
       "  'token_str': 'pozytywny',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był do nieczego, jestem zachwycony tym, że można opowiadać w tak obojętny dla słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie pozytywny'},\n",
       " {'score': 0.08461660146713257,\n",
       "  'token': 5868,\n",
       "  'token_str': 'inny',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był do nieczego, jestem zachwycony tym, że można opowiadać w tak obojętny dla słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie inny'},\n",
       " {'score': 0.0798211544752121,\n",
       "  'token': 1899,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był do nieczego, jestem zachwycony tym, że można opowiadać w tak obojętny dla słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie.'},\n",
       " {'score': 0.07572205364704132,\n",
       "  'token': 11470,\n",
       "  'token_str': 'lepszy',\n",
       "  'sequence': 'Wykład profesora Iksińskiego był do nieczego, jestem zachwycony tym, że można opowiadać w tak obojętny dla słuchacza sposób. Wydźwięk tej wypowiedzi jest zdecydowanie lepszy'}]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "herbert_fm(prompt_7 + \" <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"inny\" / \"pozytywny\" / \"lepszy\" - model  sam nie może się zdecydować, co pokrywa się z wydźwiękiem samej wypowiedzi :D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 8\n",
    "\n",
    "Answer the following questions (2 points):\n",
    "\n",
    "1. Which of the models produced the best results?\n",
    "\n",
    "Nie prowadziłem, żadnych statystyk, ale bardzo subiektywnie wskazuję na `herBERT`. Zdaje się, że nieco lepiej wykrywał sentyment.\n",
    "Na moją ocenę może mieć wpływ również to, że `herBERT` miał możliwość korzystania z `fill-mask` a \"papuga\" nie. Może warto było również sprawdzić jak `herBERT` radziłby sobie\n",
    "z uzupełnianiem maski wewnątrz wypowiedzi, a nie tylko na jej skraju.\n",
    "\n",
    "\n",
    "2. Was any of the models able to capture Polish grammar?\n",
    "\n",
    "Tak, obydwa modele potrafiły dopasować swoją \"fantazję\" (wygenerowany output) do kontekstu wypowiedzi, aby ta była poprawna gramatycznie -- a przynajmniej bezpośrednia kontynuuacja. Im większy rozmiar fantazji (outputu), tym łatwiej było o błędy i zgubienie kontekstu.\n",
    "Problemy miałem jedynie z wołaczem, ale tutaj bardzo prawdopodobne, że moje prompty nie były najlepsze.\n",
    "\n",
    "\n",
    "3. Was any of the models able to capture long-distant relationships between the words?\n",
    "\n",
    "Tak, testowałem tutaj przykłady z płcią podmiotu i dopasowaniem odpowiedniej formy czasownika ->  obydwa modele, w ramach tych krótkich wypowiedzi, którymi je raczyłem, potrafiły doposować się do kontekstu.\n",
    "Podobnie jak w 2: zdarzało się, że dla dłuższej fantazji ta wiedza była już gubiona + w przypadku \"papugi\" skuteczność była mniejsza (mniejszy odsetek poprawnie gramatycznych odpowiedzi).\n",
    "\n",
    "Zwracam też uwagę, że często o ile same wypowiedzi były jako tako poprawne gramatycznie, to \"sensu\" nie miały.\n",
    "\n",
    "\n",
    "4. Was any of the models able to capture world knowledge?\n",
    "\n",
    "W przypadku obu modeli, dla moich pytań, żaden z nich nie podał poprafnych faktograficznie odpowiedzi. Możliwe, że moje pytania były na tyle specyficzne, że korpus na którym były trenowane nie zawierał tych danych. \n",
    "Z tego co widziałem w dokumentacji \"papugi\" to potrafi ona zwrócić sensowny output dla pytań typu: \"najlepszym polskim poetą był\". Dla `herBERT` nie mogę wysnuć takich wniosków.\n",
    "\n",
    "5. Was any of the models good at doing zero-shot classification?\n",
    "\n",
    "Zwracam uwagę, że użyta przeze mnie fraza `wydźwięk tej wypowiedzi jest zdecydowanie` jest specyficzna -- intuicyjnie bardzo często występuje razem (mieli byśmy tutaj wysokie PMI da 3/4-gramów), więc ta klasyfikacja nieznanych wcześniej kategorii może być tutaj zaburzona.\n",
    "W przypadku wyraźnych \"sentymentów\" w wypowiedzi oba modele radziły sobie dobrze (`herBERT` lepiej). W przypadku ambiwalentnych wypowdzi, bądź po prostu neutralnych było gorzej w przypadku obu modeli.\n",
    "\n",
    "6. What are the most striking errors made by the models?\n",
    "\n",
    "Przedewszystkim faktograficzne błędy (widoczne w wynikach wyżej), zwracałem na nie uwagę na bieżąco.\n",
    "\"Papuga\" też bardzo często \"bredziła\" zupełnie bez sensu, choć poprawnie gramatycznie. W przypadku `herBERT`a tego nie obserwowałem, ale korzystałem z niego jedynie w trybie `fill-mask`, nie generując dłuższych fantazji."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
