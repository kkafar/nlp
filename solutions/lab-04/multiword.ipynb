{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15805292",
   "metadata": {},
   "source": [
    "# Kacper Kafara\n",
    "\n",
    "## Lab 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1846e62",
   "metadata": {},
   "source": [
    "# Uwaga\n",
    "\n",
    "Hash ostatniego commita w repo z zadaniami gdy pobrałem tekst zadań: `e3d6681`\n",
    "\n",
    "Link do ostatniego commita: [link](https://github.com/apohllo/nlp/commit/e3d6681744adf3dd9025ce5386ab622a30c7e42f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de4857",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "\n",
    "*Use SpaCy tokenizer API to tokenize the text from the FIQA corpus (from the 1 lab).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc55966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkafara/.cache/virtualenvs/python-3.11-venv-ds/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import textacy\n",
    "import datasets as ds\n",
    "\n",
    "dev_text_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e77c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'clarin-knext/fiqa-pl'\n",
    "dataset_name = 'corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ad69e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'title', 'text'],\n",
       "    num_rows: 57638\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiqa_ds = ds.load_dataset(dataset_path, dataset_name)[dataset_name]\n",
    "fiqa_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a8a266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nie mówię, że nie podoba mi się też pomysł szkolenia w miejscu pracy, ale nie możesz oczekiwać, że firma to zrobi. Szkolenie pracowników to nie ich praca – oni tworzą oprogramowanie. Być może systemy edukacyjne w Stanach Zjednoczonych (lub ich studenci) powinny trochę martwić się o zdobycie umiejętności rynkowych w zamian za ich ogromne inwestycje w edukację, zamiast wychodzić z tysiącami zadłużonych studentów i narzekać, że nie są do niczego wykwalifikowani.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = fiqa_ds['text']\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f5473",
   "metadata": {},
   "source": [
    "Jako, że nie widziałem możliwości, aby zmodyfikować tekst w Tokenie (a potrzebujemy zrobić lowercase),\n",
    "więc konwertuję najpierw cały tekst, a potem tokenizuję."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f318cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nie mówię, że nie podoba mi się też pomysł szkolenia w miejscu pracy, ale nie możesz oczekiwać, że firma to zrobi. szkolenie pracowników to nie ich praca – oni tworzą oprogramowanie. być może systemy edukacyjne w stanach zjednoczonych (lub ich studenci) powinny trochę martwić się o zdobycie umiejętności rynkowych w zamian za ich ogromne inwestycje w edukację, zamiast wychodzić z tysiącami zadłużonych studentów i narzekać, że nie są do niczego wykwalifikowani.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, text in enumerate(texts):\n",
    "    texts[i] = text.lower()\n",
    "\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68c2f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer dla j. polskiego\n",
    "spacy_nlp = spacy.load('pl_core_news_sm')\n",
    "tokenizer = spacy_nlp.tokenizer\n",
    "\n",
    "# Tokenizacja\n",
    "tokenized_texts = list(tokenizer.pipe(texts, batch_size=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c72bb",
   "metadata": {},
   "source": [
    "# Zadanie 2\n",
    "\n",
    "*Compute bigram counts of downcased tokens*\n",
    "\n",
    "Korzystam z `textacy`. Nie znam tej biblioteki, trzymam kciuki, że działa dobrze.\n",
    "\n",
    "Potrzebuję stworzyć `SpanWrapper`, bo tokenizer + `textacy` zwracają `Span`, który jest haszowany z wykorzystaniem doca, i pozycji tokenów w tym docu ==> nawet jeżeli tokeny mają taki sam tekst,\n",
    "to jeśli są w różnych miejsach w tekście (czyli 100% przypadków) to mają różne hashe ==> zliczanie słownikiem nie działa\n",
    "\n",
    "Referencja w kodzie spaCy: https://github.com/adrianeboyd/spaCy/blob/120601ddbe675a6acc5cfa2900b48a1fe4b23399/spacy/tokens/span.pyx#L149-L150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db52c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "class SpanWrapper:\n",
    "    def __init__(self, bigram: spacy.tokens.span.Span):\n",
    "        self.bigram = bigram\n",
    "        self.hash_value = hash(str(bigram))\n",
    "\n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        return self.hash_value == other.hash_value\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return self.hash_value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.bigram)\n",
    "\n",
    "def wrap_spans(span: spacy.tokens.span.Span) -> SpanWrapper:\n",
    "    return SpanWrapper(span)\n",
    "\n",
    "# Łączę wszystkie teksty w jeden duży, bo dodawanie słowników jest niesamowicie powolne...\n",
    "joined_doc = spacy.tokens.doc.Doc.from_docs(tokenized_texts)\n",
    "del tokenized_texts\n",
    "\n",
    "\n",
    "bigram_counter = Counter(map(wrap_spans, textacy.extract.basics.ngrams(joined_doc, n=2, min_freq=1, filter_stops=False, filter_punct=False, filter_nums=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "793e51ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(, że, 86093), (, a, 32643), (, ale, 32549), (, aby, 28290), (, które, 22362), (. jeśli, 20292), (, co, 18674), (to,, 16612), (, jeśli, 15689), (jest to, 15642)]\n",
      "2039426\n"
     ]
    }
   ],
   "source": [
    "def most_common_bigrams_from_counter(counter: Counter, n: int = 10) -> list[tuple[SpanWrapper, int]]:\n",
    "    return list(map(lambda swrap: (swrap[0].bigram, swrap[1]), counter.most_common(n)))\n",
    "\n",
    "most_common_bigrams = most_common_bigrams_from_counter(bigram_counter, 10)\n",
    "print(most_common_bigrams)\n",
    "print(len(bigram_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c706575",
   "metadata": {},
   "source": [
    "# Zadanie 3\n",
    "\n",
    "*Discard bigrams containing characters other than letters. Make sure that you discard the invalid entries after computing the bigram counts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "634cdcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(jest to, 15642),\n",
       "  (nie jest, 11242),\n",
       "  (po prostu, 9762),\n",
       "  (nie ma, 8626),\n",
       "  (w tym, 7808),\n",
       "  (się z, 6060),\n",
       "  (i nie, 5203),\n",
       "  (w przypadku, 5137),\n",
       "  (się na, 5133),\n",
       "  (że nie, 5012)],\n",
       " 1568709)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_alpha_or_whitespace(string: str) -> bool:\n",
    "    return all(map(lambda x: x.isalpha() or x.isspace(), string))\n",
    "\n",
    "letter_only_bigram_counts = Counter({\n",
    "    swrap: count for swrap, count in bigram_counter.items() if is_alpha_or_whitespace(swrap.bigram.text)\n",
    "})\n",
    "\n",
    "most_common_bigrams_from_counter(letter_only_bigram_counts), len(letter_only_bigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3b8c9",
   "metadata": {},
   "source": [
    "# Zadanie 4\n",
    "\n",
    "*Use pointwise mutual information to compute the measure for all pairs of words.*\n",
    "\n",
    "wg Wikipedii PMI to:\n",
    "\n",
    "![Alt text](image.png)\n",
    "\n",
    "Jako, że nie jestem pewien jak to powinno być liczone, to przyjmuję następująco:\n",
    "\n",
    "Przestrzeń zdarzeń to zbiór wszystkich bigramów.\n",
    "\n",
    "Wtedy p(x) określam jako prawdopodobieństwo wystąpienia słowa x w bigramie --> (liczba bigramów w których jest x) / (liczba wszystkich bigramów).\n",
    "\n",
    "p(x, y) określam jako wystąpienia bigramu \"x y\" przez liczbę wszystkich bigramów\n",
    "\n",
    "Dodatkowo jako \"liczbę wszystkich bigramów\" przyjmuję liczbę bigramów **przed odfiltrowaniem** tych bigramów, które nie zawierają tylko liter (i spacji pomiędzy). Tak sugeruje mi drugie zdanie w treści zadania 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa923e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1568709/1568709 [00:05<00:00, 307962.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<__main__.SpanWrapper object at 0x7fc3b820b5d0>, 3.2934839651841195),\n",
      " (<__main__.SpanWrapper object at 0x7fc3a844d290>, 1.0797346306395657),\n",
      " (<__main__.SpanWrapper object at 0x7fc3a844ff10>, 3.0118232161899385),\n",
      " (<__main__.SpanWrapper object at 0x7fc3a844ff50>, 7.982189514658092),\n",
      " (<__main__.SpanWrapper object at 0x7fc3a844fe50>, 2.8144940094670368),\n",
      " (<__main__.SpanWrapper object at 0x7fc3a844fbd0>, -0.5551592742439677),\n",
      " (<__main__.SpanWrapper object at 0x7fc3a844d950>, 0.9311125432685401),\n",
      " (<__main__.SpanWrapper object at 0x7fc3a844fe10>, 4.460903890284698),\n",
      " (<__main__.SpanWrapper object at 0x7fc3a844c790>, -0.2073479719979474),\n",
      " (<__main__.SpanWrapper object at 0x7fc3a844ca10>, 1.7038223509878065)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from math import log2\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "def prob_bigram(span_wrap: SpanWrapper, bigram_counter: Counter, bigrams_count: int) -> float:\n",
    "    return bigram_counter[span_wrap] / bigrams_count\n",
    "\n",
    "    \n",
    "def pmi(span_wrap: SpanWrapper, bigram_counter: Counter, bigrams_count: int, word_occurences: Dict[str, int]) -> float:\n",
    "    words = span_wrap.bigram.text.split(' ')\n",
    "    assert len(words) == 2, f\"Expected two words in bigram, got {words}\"\n",
    "    return log2(prob_bigram(span_wrap, bigram_counter, bigrams_count)) - log2((word_occurences[words[0]] /  bigrams_count) * (word_occurences[words[1]] / bigrams_count))\n",
    "\n",
    "bigrams_count = bigram_counter.total()\n",
    "word_occurences = defaultdict(int)\n",
    "\n",
    "# Tutaj zliczam tylko słowa, które będę potem odpytywał.\n",
    "# Korzystam z count z przefiltrowanych bigramów, bo inaczej\n",
    "# robi się to ciężkie obliczeniowo.\n",
    "# Liczę, że za bardzo się to nie rozjedzie.\n",
    "for span_wrap, count in letter_only_bigram_counts.items():\n",
    "    words = span_wrap.bigram.text.split(' ')\n",
    "    word_occurences[words[0]] += count\n",
    "    word_occurences[words[1]] += count\n",
    "\n",
    "pmi_map = {\n",
    "    span_wrap: pmi(span_wrap, bigram_counter, bigrams_count, word_occurences) for span_wrap in tqdm(letter_only_bigram_counts.keys())\n",
    "}\n",
    "\n",
    "pprint(list(pmi_map.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532de6ec",
   "metadata": {},
   "source": [
    "# Zadania 5, 6\n",
    "\n",
    "*Sort the word pairs according to that measure in the descending order and determine top 10 entries.*\n",
    "\n",
    "*Filter bigrams with number of occurrences lower than 5. Determine top 10 entries for the remaining dataset (>=5 occurrences).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb23fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[али устройстройството,\n",
      " program킨 diegimas,\n",
      " skambinkite mums,\n",
      " przetwarzającemu przejmującemu,\n",
      " finest liquids,\n",
      " роект btchash,\n",
      " попробуй сам,\n",
      " marriner eccles,\n",
      " cort ammon,\n",
      " zjebali mroźny]\n"
     ]
    }
   ],
   "source": [
    "pmi_list = list(pmi_map.items())\n",
    "pmi_list.sort(key=lambda entry: entry[1], reverse=True)\n",
    "pprint(list(map(lambda entry: entry[0].bigram, pmi_list[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08921039",
   "metadata": {},
   "source": [
    "\n",
    "Teraz odfiltorwujemy bigramy o małej liczbie wystąpień."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "549b935d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[моя группа,\n",
      " autot ldr,\n",
      " которые платят,\n",
      " остались вопросы,\n",
      " инарные опционы,\n",
      " олимп трейд,\n",
      " мою команду,\n",
      " nonresident external,\n",
      " lightning eliminators,\n",
      " stucco veneziano]\n"
     ]
    }
   ],
   "source": [
    "pmi_list = [entry for entry in pmi_list if letter_only_bigram_counts[entry[0]] >= 5]\n",
    "no_lt_result = [entry[0].bigram for entry in pmi_list][:10]\n",
    "pprint(no_lt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e59a60",
   "metadata": {},
   "source": [
    "Tak samo. \"metali szlachetnych\", \"due diligence\", \"klęski żywiołowe\", \"brzytwa Ockhama\" -- te zwroty rzeczywiście występują często razem w j. polskim ==> wygląda do sensownie.\n",
    "\n",
    "^ -- powyższe dostałem gdy nie puściłem liczenia na całym korpusie, a na pierwszych 10k tekstach...\n",
    "\n",
    "Na całym korpisue wyniki zostały \"zaśmiecone\" wstawkami z innych języków!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d2631",
   "metadata": {},
   "source": [
    "# Zadanie 7\n",
    "\n",
    "*Use SpaCy to lemmatize and tag the sentences in the corpus.*\n",
    "\n",
    "Przepuszczam nasze teksty poprostu przez cały Pipeline dla polskiego, jako że w \"środku\" jest lematyzacja i tagowanie: https://spacy.io/models/pl#pl_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4466d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = list(spacy_nlp.pipe(texts, batch_size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73824676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nie QUB nie\n",
      "mówię FIN mówić\n"
     ]
    }
   ],
   "source": [
    "for token in processed_texts[0][:2]:\n",
    "    print(token, token.tag_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e8cd8",
   "metadata": {},
   "source": [
    "^ Wygląda sensownie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f61fa",
   "metadata": {},
   "source": [
    "# Zadanie 8\n",
    "\n",
    "Using the tagged corpus compute bigram statistic for the tokens containing: \n",
    "* a. lemmatized, downcased word \n",
    "* b. morphosyntactic category of the word (subst, fin, adj, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e3110",
   "metadata": {},
   "source": [
    "Tworzymy bigramy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11ee8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "class SpanWrapperLT:\n",
    "    def __init__(self, bigram: spacy.tokens.span.Span):\n",
    "        self.bigram = bigram\n",
    "        self.hash_string = \" \".join([f'{token.lemma_}:{token.tag_}' for token in self.bigram])\n",
    "        self.hash_value = hash(self.hash_string)\n",
    "\n",
    "    def tokens(self):\n",
    "        return (self.bigram.doc[self.bigram.start], self.bigram.doc[self.bigram.end-1])\n",
    "\n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        return self.hash_value == other.hash_value\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return self.hash_value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.hash_string\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.__str__()\n",
    "\n",
    "def wrap_spans_lt(span: spacy.tokens.span.Span) -> SpanWrapperLT:\n",
    "    return SpanWrapperLT(span)\n",
    "\n",
    "# Łączę wszystkie teksty w jeden duży, bo dodawanie słowników jest niesamowicie powolne...\n",
    "joined_doc = spacy.tokens.doc.Doc.from_docs(processed_texts)\n",
    "# del processed_texts\n",
    "\n",
    "\n",
    "bigram_lt_counter = Counter(map(wrap_spans_lt, textacy.extract.basics.ngrams(joined_doc, n=2, min_freq=1, filter_stops=False, filter_punct=False, filter_nums=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45446b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(, że, 75938),\n",
      " (, które, 53518),\n",
      " (, a, 32492),\n",
      " (, ale, 31389),\n",
      " (to,, 26532),\n",
      " (, aby, 26512),\n",
      " (. jeśli, 18613),\n",
      " (nie są, 18313),\n",
      " (, jeśli, 15328),\n",
      " (, ponieważ, 14998)]\n",
      "1801315\n"
     ]
    }
   ],
   "source": [
    "most_common_bigrams_lt = most_common_bigrams_from_counter(bigram_lt_counter, 10)\n",
    "pprint(most_common_bigrams_lt)\n",
    "print(len(bigram_lt_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de07e4c9",
   "metadata": {},
   "source": [
    "Odfiltrowuję bigramy zawierające znaki inne niż litery (+ spacja) i liczę PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cc85312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(nie są, 18313),\n",
       "  (nie ma, 13376),\n",
       "  (jest to, 11298),\n",
       "  (po prostu, 9579),\n",
       "  (w tych, 9005),\n",
       "  (mogą być, 7178),\n",
       "  (się z, 6631),\n",
       "  (nie możesz, 6594),\n",
       "  (w którym, 6584),\n",
       "  (na to, 6487)],\n",
       " 1306985)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_only_bigram_lt_counts = Counter({\n",
    "    swrap: count for swrap, count in bigram_lt_counter.items() if is_alpha_or_whitespace(swrap.bigram.text)\n",
    "})\n",
    "\n",
    "most_common_bigrams_from_counter(letter_only_bigram_lt_counts), len(letter_only_bigram_lt_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0d14670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306985/1306985 [00:04<00:00, 319511.56it/s]\n"
     ]
    }
   ],
   "source": [
    "bigrams_count = bigram_lt_counter.total()\n",
    "word_occurences = defaultdict(int)\n",
    "\n",
    "for span_wrap, count in letter_only_bigram_lt_counts.items():\n",
    "    words = span_wrap.bigram.text.split(' ')\n",
    "    word_occurences[words[0]] += count\n",
    "    word_occurences[words[1]] += count\n",
    "\n",
    "pmi_lt_map = {\n",
    "    span_wrap: pmi(span_wrap, bigram_lt_counter, bigrams_count, word_occurences) for span_wrap in tqdm(letter_only_bigram_lt_counts.keys())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d706fff",
   "metadata": {},
   "source": [
    "Sortuję i wypisuję bigramy o największym PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe298eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_lt_list = list(pmi_lt_map.items())\n",
    "pmi_lt_list.sort(key=lambda entry: entry[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c5f4b",
   "metadata": {},
   "source": [
    "Odfiltrowuję bigramy o niskiej liczbie wystąpień"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f5a3cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[моя группа,\n",
      " autot ldr,\n",
      " rurociągami naftowymi,\n",
      " biegłymi rewidentami,\n",
      " остались вопросы,\n",
      " которые платят,\n",
      " инарные опционы,\n",
      " олимп трейд,\n",
      " мою команду,\n",
      " skuterze śnieżnym]\n"
     ]
    }
   ],
   "source": [
    "pmi_lt_list = [entry for entry in pmi_lt_list if letter_only_bigram_lt_counts[entry[0]] >= 5]\n",
    "lt_result = [entry[0].bigram for entry in pmi_lt_list][:10]\n",
    "pprint(lt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612d31a",
   "metadata": {},
   "source": [
    "# Zadania 11, 12\n",
    "\n",
    "*Group the bigrams by morphosyntactic tag, i.e. a pair of words belongs to a given group if all pairs have the same syntactic category for the first and the second word.\n",
    "E.g. one group would be words with subst as the first words and adj as the second word.*\n",
    "\n",
    "*Print top-10 categories (sort them by total count of bigrams) and print top-5 pairs for each category.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc2d33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupKey:\n",
    "    def __init__(self, wrapper: SpanWrapperLT):\n",
    "        self.wrapper = wrapper\n",
    "        token_1, token_2 = wrapper.tokens()\n",
    "        self.hash_string = token_1.tag_ + ':' + token_2.tag_\n",
    "        self.hash_value = hash(self.hash_string)\n",
    "\n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        return self.hash_string == other.hash_string\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.hash_value\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return self.hash_string\n",
    "        \n",
    "keyed_wrappers = [GroupKey(wrap) for wrap in letter_only_bigram_lt_counts.keys()]\n",
    "\n",
    "grouped_by_tags = defaultdict(list)\n",
    "for keyed_wrapper in keyed_wrappers:\n",
    "    # print(keyed_wrapper)\n",
    "    grouped_by_tags[keyed_wrapper].append(keyed_wrapper.wrapper) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e572a2",
   "metadata": {},
   "source": [
    "Top 10 kategorii:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3894d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBST:SUBST [miejscu pracy, niczego wykwalifikowani, strony rynku, ramach dfa, dfa banki]\n",
      "ADJ:SUBST [ogromne inwestycje, fałszywym ocenom, dodatkową kontrolą, nowsze kontrole, należytej staranności]\n",
      "SUBST:ADJ [systemy edukacyjne, stanach zjednoczonych, umiejętności rynkowych, ratingach kredytowych, instrumentu finansowego]\n",
      "SUBST:FIN [intencją jest, cdo jest, fsa mogą, co chcą, składek jest]\n",
      "PREP:SUBST [w miejscu, w stanach, w edukację, z tysiącami, do niczego]\n",
      "FIN:SUBST [tworzą oprogramowanie, może systemy, uniemożliwiają instytucjom, jest plus, jest to]\n",
      "SUBST:PREP [inwestycje w, ocenom poza, kontrolą ze, staranności przy, fsa dla]\n",
      "INF:SUBST [ograniczyć to, użyć fsa, użyć planu, kupić kontrakt, dokonać dostawy]\n",
      "CONJ:SUBST [i narzekać, więc nic, to samsung, lub firma, lub spółka]\n",
      "GER:SUBST [szkolenie pracowników, zdobycie umiejętności, opłacania składek, wyłączeniem wartości, zachowanie instrumentów]\n"
     ]
    }
   ],
   "source": [
    "items_by_tag = list(grouped_by_tags.items())\n",
    "items_by_tag.sort(key=lambda kv: len(kv[1]), reverse=True)\n",
    "for cat, items in items_by_tag[:10]:\n",
    "    print(cat, [item.bigram for item in items[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5315a",
   "metadata": {},
   "source": [
    "# Zadanie 13\n",
    "\n",
    "*Create a table comparing the results for copora without and with tagging and lemmatization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19ca69fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_nr</th><th>w/o lemmatization and taggging</th><th>w/ lemmatization and tagging</th></tr><tr><td>u32</td><td>object</td><td>object</td></tr></thead><tbody><tr><td>0</td><td>моя группа</td><td>моя группа</td></tr><tr><td>1</td><td>autot ldr</td><td>autot ldr</td></tr><tr><td>2</td><td>которые платят</td><td>rurociągami naftowymi</td></tr><tr><td>3</td><td>остались вопросы</td><td>biegłymi rewidentami</td></tr><tr><td>4</td><td>инарные опционы</td><td>остались вопросы</td></tr><tr><td>5</td><td>олимп трейд</td><td>которые платят</td></tr><tr><td>6</td><td>мою команду</td><td>инарные опционы</td></tr><tr><td>7</td><td>nonresident external</td><td>олимп трейд</td></tr><tr><td>8</td><td>lightning eliminators</td><td>мою команду</td></tr><tr><td>9</td><td>stucco veneziano</td><td>skuterze śnieżnym</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 3)\n",
       "┌────────┬────────────────────────────────┬──────────────────────────────┐\n",
       "│ row_nr ┆ w/o lemmatization and taggging ┆ w/ lemmatization and tagging │\n",
       "│ ---    ┆ ---                            ┆ ---                          │\n",
       "│ u32    ┆ object                         ┆ object                       │\n",
       "╞════════╪════════════════════════════════╪══════════════════════════════╡\n",
       "│ 0      ┆ моя группа                     ┆ моя группа                   │\n",
       "│ 1      ┆ autot ldr                      ┆ autot ldr                    │\n",
       "│ 2      ┆ которые платят                 ┆ rurociągami naftowymi        │\n",
       "│ 3      ┆ остались вопросы               ┆ biegłymi rewidentami         │\n",
       "│ …      ┆ …                              ┆ …                            │\n",
       "│ 6      ┆ мою команду                    ┆ инарные опционы              │\n",
       "│ 7      ┆ nonresident external           ┆ олимп трейд                  │\n",
       "│ 8      ┆ lightning eliminators          ┆ мою команду                  │\n",
       "│ 9      ┆ stucco veneziano               ┆ skuterze śnieżnym            │\n",
       "└────────┴────────────────────────────────┴──────────────────────────────┘"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "pl.DataFrame({\n",
    "    'w/o lemmatization and taggging': no_lt_result,\n",
    "    'w/ lemmatization and tagging': lt_result\n",
    "}).with_row_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbd2cba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_nr</th><th>SUBST:SUBST</th><th>ADJ:SUBST</th><th>SUBST:ADJ</th><th>SUBST:FIN</th><th>PREP:SUBST</th><th>FIN:SUBST</th><th>SUBST:PREP</th><th>INF:SUBST</th><th>CONJ:SUBST</th><th>GER:SUBST</th></tr><tr><td>u32</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td><td>object</td></tr></thead><tbody><tr><td>0</td><td>miejscu pracy</td><td>ogromne inwestycje</td><td>systemy edukacyjne</td><td>intencją jest</td><td>w miejscu</td><td>tworzą oprogramowanie</td><td>inwestycje w</td><td>ograniczyć to</td><td>i narzekać</td><td>szkolenie pracowników</td></tr><tr><td>1</td><td>niczego wykwalifikowani</td><td>fałszywym ocenom</td><td>stanach zjednoczonych</td><td>cdo jest</td><td>w stanach</td><td>może systemy</td><td>ocenom poza</td><td>użyć fsa</td><td>więc nic</td><td>zdobycie umiejętności</td></tr><tr><td>2</td><td>strony rynku</td><td>dodatkową kontrolą</td><td>umiejętności rynkowych</td><td>fsa mogą</td><td>w edukację</td><td>uniemożliwiają instytucjom</td><td>kontrolą ze</td><td>użyć planu</td><td>to samsung</td><td>opłacania składek</td></tr><tr><td>3</td><td>ramach dfa</td><td>nowsze kontrole</td><td>ratingach kredytowych</td><td>co chcą</td><td>z tysiącami</td><td>jest plus</td><td>staranności przy</td><td>kupić kontrakt</td><td>lub firma</td><td>wyłączeniem wartości</td></tr><tr><td>4</td><td>dfa banki</td><td>należytej staranności</td><td>instrumentu finansowego</td><td>składek jest</td><td>do niczego</td><td>jest to</td><td>fsa dla</td><td>dokonać dostawy</td><td>lub spółka</td><td>zachowanie instrumentów</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 11)\n",
       "┌────────┬────────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ row_nr ┆ SUBST:SUBS ┆ ADJ:SUBST  ┆ SUBST:ADJ ┆ … ┆ SUBST:PRE ┆ INF:SUBST ┆ CONJ:SUBS ┆ GER:SUBST │\n",
       "│ ---    ┆ T          ┆ ---        ┆ ---       ┆   ┆ P         ┆ ---       ┆ T         ┆ ---       │\n",
       "│ u32    ┆ ---        ┆ object     ┆ object    ┆   ┆ ---       ┆ object    ┆ ---       ┆ object    │\n",
       "│        ┆ object     ┆            ┆           ┆   ┆ object    ┆           ┆ object    ┆           │\n",
       "╞════════╪════════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 0      ┆ miejscu    ┆ ogromne    ┆ systemy   ┆ … ┆ inwestycj ┆ ograniczy ┆ i         ┆ szkolenie │\n",
       "│        ┆ pracy      ┆ inwestycje ┆ edukacyjn ┆   ┆ e w       ┆ ć to      ┆ narzekać  ┆ pracownik │\n",
       "│        ┆            ┆            ┆ e         ┆   ┆           ┆           ┆           ┆ ów        │\n",
       "│ 1      ┆ niczego    ┆ fałszywym  ┆ stanach   ┆ … ┆ ocenom    ┆ użyć fsa  ┆ więc nic  ┆ zdobycie  │\n",
       "│        ┆ wykwalifik ┆ ocenom     ┆ zjednoczo ┆   ┆ poza      ┆           ┆           ┆ umiejętno │\n",
       "│        ┆ owani      ┆            ┆ nych      ┆   ┆           ┆           ┆           ┆ ści       │\n",
       "│ 2      ┆ strony     ┆ dodatkową  ┆ umiejętno ┆ … ┆ kontrolą  ┆ użyć      ┆ to        ┆ opłacania │\n",
       "│        ┆ rynku      ┆ kontrolą   ┆ ści       ┆   ┆ ze        ┆ planu     ┆ samsung   ┆ składek   │\n",
       "│        ┆            ┆            ┆ rynkowych ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 3      ┆ ramach dfa ┆ nowsze     ┆ ratingach ┆ … ┆ starannoś ┆ kupić     ┆ lub firma ┆ wyłączeni │\n",
       "│        ┆            ┆ kontrole   ┆ kredytowy ┆   ┆ ci przy   ┆ kontrakt  ┆           ┆ em        │\n",
       "│        ┆            ┆            ┆ ch        ┆   ┆           ┆           ┆           ┆ wartości  │\n",
       "│ 4      ┆ dfa banki  ┆ należytej  ┆ instrumen ┆ … ┆ fsa dla   ┆ dokonać   ┆ lub       ┆ zachowani │\n",
       "│        ┆            ┆ starannośc ┆ tu finans ┆   ┆           ┆ dostawy   ┆ spółka    ┆ e instrum │\n",
       "│        ┆            ┆ i          ┆ owego     ┆   ┆           ┆           ┆           ┆ entów     │\n",
       "└────────┴────────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.DataFrame({\n",
    "    str(col): [item.bigram for item in items[:5]] for col, items in items_by_tag[:10]\n",
    "}).with_row_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ebda6f",
   "metadata": {},
   "source": [
    "# Zadanie 14\n",
    "\n",
    "Answer the following questions:\n",
    "* Why do we have to filter the bigrams, rather than the token sequence?\n",
    "* What types of expressions are discovered by the methods.\n",
    "* Can you devise a different type of filtering that would yield better results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b904cc7f",
   "metadata": {},
   "source": [
    "* Filtrujemy bigramy o niskiej liczbie wystąpień, ponieważ chcemy wyeliminować sytuacje w których pojedyncze zestawienia słów uzysukją bardzo duży wskaźnik PMI w dość przypadkowy sposób. Nie za bardzo rozumiem\n",
    "o jakie \"filtrowanie sekwencji tokenów\" chodziło autorowi pytania, więc pomijam.\n",
    "\n",
    "* Zakładam, że \"metody\" to bez lematyzacji + tagownia oraz z. Moje ogólne wrażenie jest takie, ze obie te metody faktycznie wskazały te bigramy, które faktycznie często występują wspólnie w języku polskim (\"naturalne kolokacje\"?).\n",
    "  Część pozycji w top 10 obu metod pokrywa się (na różnych miejscach, ale te same wartości): \"litr mleka\", \"dyrektor generalny\", \"rezerwa federalna\", \"instrumenty pochodne\".\n",
    "\n",
    "* Zwiększenie limitu liczby wystąpień (10, 20, 30)?. Też zasadne jest pytanie, co znaczy \"lepszy wynik\" tutaj?\n",
    "\n",
    "\n",
    "**UWAGA** Powyższe odpowiedzi napisałem puszczając obliczenia dla pierwszych 10k tekstów z korpusu. Wtedy wygląda to naprawdę fajnie. Gdy puściłem na całości, wyniki zostały zupełnie zniekształcone i zaśmiecone,\n",
    "w szczególności wstawkami z innych języków.\n",
    "\n",
    "Możnaby zatem próbować w \"jakiś sposób\" (korzystając z lematyzacji możemy rozpoznawać czy fraza jest po polsku czy nie, dosyc łatwo (są słowniki)) filtrować wyrażenia w obcym języku (jeżeli zależy nam na wynikach dla j. polskiego).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
